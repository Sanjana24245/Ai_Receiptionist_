import ollama

def get_ai_reply(user_message: str) -> str:
    try:
        response = ollama.chat(
            model="llama3:latest",
            messages=[{"role": "user", "content": user_message}]
        )
        return response["message"]["content"]
    except Exception as e:
        print("âŒ Ollama error:", e)
        return "Sorry, Iâ€™m having trouble answering right now."

if __name__ == "__main__":
    reply = get_ai_reply("Hello, who are you?")
    print("ğŸ¤– AI Reply:", reply)